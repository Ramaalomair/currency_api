    from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from PIL import Image
import io
import joblib
import torch
import torchvision.transforms as transforms
from torchvision import models
import numpy as np
import logging
import os
import cv2

# ========================================
# LOGGING
# ========================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ========================================
# APP & CONFIG
# ========================================
app = FastAPI(title="Saudi Currency Recognition API")

MODEL_PATH = "models/currency/FINAL_SVM_(RBF).pkl"

# ========================================
# CURRENCY MAPPING
# ========================================
CURRENCY_NAMES = {
    0: "5 SR", 1: "10 SR", 2: "20 SR", 3: "50 SR",
    4: "100 SR", 5: "200 SR", 6: "500 SR"
}

CURRENCY_TEXT_AR = {
    "5 SR": "Ø®Ù…Ø³Ø© Ø±ÙŠØ§Ù„Ø§Øª Ø³Ø¹ÙˆØ¯ÙŠØ©",
    "10 SR": "Ø¹Ø´Ø±Ø© Ø±ÙŠØ§Ù„Ø§Øª Ø³Ø¹ÙˆØ¯ÙŠØ©",
    "20 SR": "Ø¹Ø´Ø±ÙˆÙ† Ø±ÙŠØ§Ù„Ø§Ù‹ Ø³Ø¹ÙˆØ¯ÙŠØ§Ù‹",
    "50 SR": "Ø®Ù…Ø³ÙˆÙ† Ø±ÙŠØ§Ù„Ø§Ù‹ Ø³Ø¹ÙˆØ¯ÙŠØ§Ù‹",
    "100 SR": "Ù…Ø¦Ø© Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ",
    "200 SR": "Ù…Ø¦ØªØ§ Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ",
    "500 SR": "Ø®Ù…Ø³Ù…Ø¦Ø© Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ",
}

# ========================================
# GLOBAL MODELS
# ========================================
mobilenet = None
svm_model = None

# ========================================
# STARTUP
# ========================================
@app.on_event("startup")
async def load_model():
    global mobilenet, svm_model
    
    logger.info("=" * 60)
    logger.info("ğŸ”„ INITIALIZING CURRENCY RECOGNITION SYSTEM")
    logger.info("=" * 60)
    
    logger.info("ğŸ“¥ Loading MobileNetV2 feature extractor...")
    mobilenet = models.mobilenet_v2(pretrained=True)
    mobilenet.classifier = torch.nn.Identity()
    mobilenet.eval()
    logger.info("âœ… MobileNetV2 loaded (1280-D features)")
    
    if not os.path.exists(MODEL_PATH):
        logger.error(f"âŒ Model file not found at {MODEL_PATH}")
        raise FileNotFoundError(f"Model not found: {MODEL_PATH}")
    
    logger.info(f"ğŸ“¥ Loading SVM model: {MODEL_PATH}")
    
    with open(MODEL_PATH, 'rb') as f:
        svm_model = joblib.load(f)
    
    logger.info("âœ… SVM Model loaded!")
    logger.info("=" * 60)
    logger.info("âœ… SYSTEM READY WITH OPENCV DETECTION!")
    logger.info("=" * 60)

# ========================================
# FEATURE EXTRACTION
# ========================================
def extract_features(image: Image.Image) -> np.ndarray:
    preprocess = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    img_tensor = preprocess(image).unsqueeze(0)
    
    with torch.no_grad():
        features = mobilenet(img_tensor)
    
    return features.squeeze().numpy()

# ========================================
# OPENCV BANKNOTE DETECTION (IMPROVED)
# ========================================
def detect_banknotes_opencv(img_array: np.ndarray) -> list:
    height, width = img_array.shape[:2]
    
    # âœ… Ø®ÙØ¶Ù†Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ù…Ø³Ø§Ø­Ø©
    min_area = (width * height) * 0.005  # 0.5% Ø¨Ø¯Ù„ 1%
    max_area = (width * height) * 0.80   # 80% Ø¨Ø¯Ù„ 95%
    
    logger.info(f"ğŸ“ Image size: {width}x{height}")
    logger.info(f"ğŸ“ Area range: {min_area:.0f} - {max_area:.0f}")
    
    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
    
    # âœ… ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¨Ø§ÙŠÙ†
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)
    
    # âœ… Adaptive threshold Ø¨Ø¯Ù„ Canny - Ø£ÙØ¶Ù„ Ù„Ù„Ø¹Ù…Ù„Ø§Øª
    thresh = cv2.adaptiveThreshold(
        enhanced, 255, 
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
        cv2.THRESH_BINARY_INV, 
        11, 2
    )
    
    # ØªÙ†Ø¸ÙŠÙ
    kernel = np.ones((5, 5), np.uint8)
    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)
    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)
    
    # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù€ contours
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    logger.info(f"ğŸ” Found {len(contours)} initial contours")
    
    detections = []
    
    for contour in contours:
        area = cv2.contourArea(contour)
        
        if area < min_area or area > max_area:
            continue
        
        rect = cv2.minAreaRect(contour)
        w, h = rect[1]
        
        if w == 0 or h == 0:
            continue
            
        if h > w:
            w, h = h, w
        
        aspect_ratio = w / h
        
        # âœ… ÙˆØ³Ø¹Ù†Ø§ Ø§Ù„Ù…Ø¯Ù‰ Ø´ÙˆÙŠ
        if 1.3 < aspect_ratio < 4.0:
            x, y, bw, bh = cv2.boundingRect(contour)
            
            # âœ… Ø²Ø¯Ù†Ø§ Ø§Ù„Ù€ padding
            padding = 10
            x1 = max(0, x - padding)
            y1 = max(0, y - padding)
            x2 = min(width, x + bw + padding)
            y2 = min(height, y + bh + padding)
            
            confidence = max(0.5, 1.0 - (abs(aspect_ratio - 2.2) * 0.15))
            
            detections.append({
                "bbox": [x1, y1, x2, y2],
                "confidence": confidence,
                "area": area
            })
            
            logger.info(f"âœ… Found: bbox=[{x1},{y1},{x2},{y2}], ratio={aspect_ratio:.2f}")
    
    # âœ… Ø®ÙØ¶Ù†Ø§ Ø§Ù„Ø¹ØªØ¨Ø©
    detections = remove_overlapping_detections(detections, iou_threshold=0.3)
    logger.info(f"ğŸ“Š Final detections: {len(detections)}")
    
    return detections


def remove_overlapping_detections(detections: list, iou_threshold: float = 0.5) -> list:
    if len(detections) <= 1:
        return detections
    
    detections = sorted(detections, key=lambda x: x["area"], reverse=True)
    kept = []
    
    for det in detections:
        x1, y1, x2, y2 = det["bbox"]
        should_keep = True
        
        for kept_det in kept:
            kx1, ky1, kx2, ky2 = kept_det["bbox"]
            
            ix1, iy1 = max(x1, kx1), max(y1, ky1)
            ix2, iy2 = min(x2, kx2), min(y2, ky2)
            
            if ix1 < ix2 and iy1 < iy2:
                intersection = (ix2 - ix1) * (iy2 - iy1)
                area1 = (x2 - x1) * (y2 - y1)
                area2 = (kx2 - kx1) * (ky2 - ky1)
                union = area1 + area2 - intersection
                
                if intersection / union > iou_threshold:
                    should_keep = False
                    break
        
        if should_keep:
            kept.append(det)
    
    return kept


# ========================================
# SINGLE RECOGNITION
# ========================================
@app.post("/recognize")
async def recognize_currency(file: UploadFile = File(...)):
    try:
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert("RGB")
        
        features = extract_features(image)
        prediction = svm_model.predict([features])[0]
        probabilities = svm_model.predict_proba([features])[0]
        confidence = float(probabilities[prediction]) * 100
        
        currency_name = CURRENCY_NAMES.get(prediction, "Unknown")
        
        return JSONResponse({
            "currency": currency_name,
            "confidence": round(confidence, 2),
            "text_ar": CURRENCY_TEXT_AR.get(currency_name, "Ø¹Ù…Ù„Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©"),
            "text_en": currency_name
        })
    
    except Exception as e:
        logger.error(f"âŒ Recognition error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========================================
# MULTI RECOGNITION WITH OPENCV
# ========================================
@app.post("/recognize-multiple")
async def recognize_multiple_currencies(file: UploadFile = File(...)):
    try:
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert("RGB")
        img_np = np.array(image)
        
        logger.info("ğŸ” Starting multi-currency recognition with OpenCV...")
        
        detections = detect_banknotes_opencv(img_np)
        
        if not detections:
            logger.warning("âš ï¸ No banknotes detected, trying full image...")
            features = extract_features(image)
            prediction = svm_model.predict([features])[0]
            probabilities = svm_model.predict_proba([features])[0]
            confidence = float(probabilities[prediction]) * 100
            
            if confidence >= 40:
                currency_name = CURRENCY_NAMES.get(prediction, "Unknown")
                value = int(currency_name.replace(" SR", "")) if currency_name != "Unknown" else 0
                
                return JSONResponse({
                    "count": 1,
                    "total": value,
                    "currencies": [{
                        "currency": currency_name,
                        "confidence": round(confidence, 2),
                        "text_ar": CURRENCY_TEXT_AR.get(currency_name, ""),
                        "text_en": currency_name
                    }],
                    "total_text_ar": f"Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ {value} Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ",
                    "total_text_en": f"Total {value} SAR"
                })
            
            return JSONResponse({
                "count": 0,
                "total": 0,
                "currencies": [],
                "total_text_ar": "Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ Ø¹Ù…Ù„Ø©",
                "total_text_en": "No currency detected"
            })
        
        currencies = []
        total_value = 0
        
        for idx, det in enumerate(detections):
            x1, y1, x2, y2 = det["bbox"]
            
            cropped = img_np[y1:y2, x1:x2]
            if cropped.size == 0:
                continue
            
            cropped_pil = Image.fromarray(cropped)
            
            features = extract_features(cropped_pil)
            prediction = svm_model.predict([features])[0]
            probabilities = svm_model.predict_proba([features])[0]
            confidence = float(probabilities[prediction]) * 100
            
            logger.info(f"ğŸ’µ Detection {idx+1}: Class={prediction}, Confidence={confidence:.1f}%")
            
            if confidence < 40:
                continue
            
            currency_name = CURRENCY_NAMES.get(prediction, "Unknown")
            value = int(currency_name.replace(" SR", "")) if currency_name != "Unknown" else 0
            
            currencies.append({
                "currency": currency_name,
                "confidence": round(confidence, 2),
                "text_ar": CURRENCY_TEXT_AR.get(currency_name, ""),
                "text_en": currency_name
            })
            
            total_value += value
        
        logger.info(f"âœ… Final: {len(currencies)} currencies, Total: {total_value} SAR")
        
        return JSONResponse({
            "count": len(currencies),
            "total": total_value,
            "currencies": currencies,
            "total_text_ar": f"Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ {total_value} Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ",
            "total_text_en": f"Total {total_value} SAR"
        })
    
    except Exception as e:
        logger.error(f"âŒ Multi-recognition error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========================================
# HEALTH CHECK
# ========================================
@app.get("/")
@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "svm_loaded": svm_model is not None,
        "detection_method": "OpenCV"
    }
